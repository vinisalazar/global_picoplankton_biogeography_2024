{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/local/path/to/data/counts/\"\n",
    "files = glob(datadir + \"*_normalised.csv\")\n",
    "files = {Path(f).stem.replace(\"_trimmed_mean_formatted_clean_normalised\", \"\"): f for f in files if \"KEGG\" in f}\n",
    "files = {k: pd.read_csv(v, index_col=0, nrows=1).columns.to_list() for k, v in files.items()}\n",
    "for k, v in files.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEGG KO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/gpfs/projects/punim1293/vini/db/kegg/genes/ko/ko', 'r', encoding=\"utf-8\", errors=\"replace\") as ko_file:\n",
    "    lines = ko_file.readlines()\n",
    "\n",
    "ko_entries = dict()\n",
    "for ix, line in enumerate(lines):\n",
    "    if line.startswith(\"ENTRY\"):\n",
    "        ko_id = line.split()[1]\n",
    "        ko_entries[ko_id] = lines[ix+2]\n",
    "\n",
    "ko_entries = {k.replace(\"ko:\", \"\"): v.replace(\"NAME        \", \"\").split(\" [EC\")[0].strip() for k, v in ko_entries.items()}\n",
    "ko_missing = [i for i in files[\"KEGG_ko\"] if i not in ko_entries.keys()]\n",
    "ko_all = {**ko_entries, **{i: i for i in ko_missing}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh for attempt 2 (April 2024)\n",
    "\n",
    "This was completed after modifying the code in the cell above this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/local/path/to/data/misc/renaming/ko_entries.tsv\", \"w\") as ko_file:\n",
    "    ko_file.write(\"ko\\tname\\n\")\n",
    "    for k, v in ko_entries.items():\n",
    "        ko_file.write(f\"{k}\\t{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing = pd.read_csv(\"/local/path/to/data/misc/renaming/ko_missing_2nd_try.csv\", header=None)[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_kegg_entry(kegg_identifier):\n",
    "    base_url = \"https://rest.kegg.jp/find/ko/\"\n",
    "    full_url = f\"{base_url}{kegg_identifier}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(full_url)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the response content (assuming it's in plain text format)\n",
    "            entry_info = response.text.strip()\n",
    "            return entry_info\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# for kegg_identifier in missing:\n",
    "#     result = search_kegg_entry(kegg_identifier)\n",
    "#     print(f\"Information for {kegg_identifier}:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = 0\n",
    "success = dict()\n",
    "\n",
    "with open(\"/local/path/to/data/misc/renaming/ko_entries_2nd_try.csv\", \"a\") as f:\n",
    "    for i, kegg_identifier in enumerate(missing):\n",
    "        if kegg_identifier in success.keys():\n",
    "            continue\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i}/{len(missing)} entries.\", end=\"\\r\")\n",
    "        try:\n",
    "            result = search_kegg_entry(kegg_identifier)\n",
    "            success[kegg_identifier] = result\n",
    "            f.write(result+\"\\n\")\n",
    "            f.flush()\n",
    "        except:\n",
    "            failed += 1\n",
    "            pass\n",
    "\n",
    "print(\"Done. Failed:\", failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEGG Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_kegg_file(file_path):\n",
    "    kegg_dict = dict()\n",
    "    current_category = None\n",
    "    current_subcategory = None\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\") and not line.startswith(\"##\"):\n",
    "                # Extract category and subcategory information\n",
    "                current_category = line[1:].strip()\n",
    "                current_subcategory = None\n",
    "                kegg_dict[current_category] = {}\n",
    "            elif line.startswith(\"##\"):\n",
    "                # Extract subcategory information\n",
    "                current_subcategory = line[2:].strip()\n",
    "                kegg_dict[current_category][current_subcategory] = {}\n",
    "            elif line:\n",
    "                # Extract pathway information\n",
    "                parts = line.split(\"\\t\")\n",
    "                kegg_id = str(parts[0])\n",
    "                pathway_name = str(parts[1])\n",
    "                if current_subcategory is not None:\n",
    "                    kegg_dict[current_category][current_subcategory][kegg_id] = pathway_name\n",
    "                else:\n",
    "                    kegg_dict[current_category][kegg_id] = pathway_name\n",
    "\n",
    "    return kegg_dict\n",
    "\n",
    "\n",
    "def find_key_in_nested_dict(nested_dict, target_key):\n",
    "    \"\"\"\n",
    "    Recursively find a target key in a nested dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        - nested_dict: The nested dictionary to search.\n",
    "        - target_key: The key to find.\n",
    "\n",
    "    Returns:\n",
    "        - The value associated with the target key, or None if not found.\n",
    "    \"\"\"\n",
    "    for key, value in nested_dict.items():\n",
    "        if key == target_key:\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            result = find_key_in_nested_dict(value, target_key)\n",
    "            if result is not None:\n",
    "                return result\n",
    "    return None\n",
    "\n",
    "kegg_pathways = parse_kegg_file(\"/data/gpfs/projects/punim1293/vini/db/kegg/pathway/pathway.list\")\n",
    "\n",
    "pathway_entries = dict()\n",
    "pathway_missing = list()\n",
    "for pathway in files[\"KEGG_Pathway\"]:\n",
    "    fmt_pathway = str(pathway[-5:])\n",
    "    if (result := find_key_in_nested_dict(kegg_pathways,fmt_pathway)) is not None:\n",
    "        pathway_entries[pathway] = result\n",
    "    else:\n",
    "        pathway_missing.append(pathway)\n",
    "\n",
    "pathway_all = {**pathway_entries, **{i: i for i in pathway_missing}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEGG RClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclass = pd.read_csv(\"/data/gpfs/projects/punim1293/vini/db/kegg/genes/ko/ko_rclass.list\", sep=\"\\t\", names=\"ko rclass\".split(), index_col=\"rclass\")\n",
    "rclass_entries = [f\"rc:{i}\" for i in files[\"KEGG_rclass\"] if f\"rc:{i}\" in rclass.index.to_list()]\n",
    "rclass_missing = [f\"rc:{i}\" for i in files[\"KEGG_rclass\"] if f\"rc:{i}\" not in rclass.index.to_list()]\n",
    "rclass_entries = rclass.loc[rclass_entries, \"ko\"].map(ko_entries)\n",
    "rclass_entries = rclass_entries.groupby(rclass_entries.index).apply(list).to_dict()\n",
    "rclass_entries = {k: [i for i in v if i == i] for k, v in rclass_entries.items()}\n",
    "rclass_all = {**rclass_entries, **{i: i for i in rclass_missing}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEGG Reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/gpfs/projects/punim1293/vini/db/kegg/brite/br/br.list\") as br_file:\n",
    "    lines = br_file.readlines()\n",
    "\n",
    "lines = {i.split()[1]: i.split()[0] for i in lines if i.startswith(\"br:\")}\n",
    "lines = {i: lines.get(\"rn:\" + i, None) for i in files[\"KEGG_Reaction\"]}\n",
    "reaction_missing = [k for k, v in lines.items() if v is None]\n",
    "lines = {k: v.replace(\"br:\", \"\") + \".keg\" for k, v in lines.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_br_file(file, reaction):\n",
    "    parentdir = \"/data/gpfs/projects/punim1293/vini/db/kegg/brite/br\"\n",
    "    with open(f\"{parentdir}/{file}\") as br_file:\n",
    "        lines = br_file.readlines()\n",
    "\n",
    "    lines = list({i for i in lines if reaction in i})\n",
    "    if len(lines) >= 1:\n",
    "        lines = lines[0][1:].strip()\n",
    "        lines = lines.replace(reaction, \"\").strip()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_entries = {k: search_br_file(v, k) for k, v in lines.items()}\n",
    "reaction_all = {**reaction_entries, **{i: i for i in reaction_missing}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in files.keys():\n",
    "    key = key[5:].lower()\n",
    "    for category in (\"_entries\", \"_missing\", \"_all\"):\n",
    "        fname = key + category\n",
    "        with open(\"/data/gpfs/projects/punim1989/biogo-hub/data/misc/renaming/\" + fname + \".tsv\", \"w\") as file:\n",
    "            if isinstance(locals()[fname], dict):\n",
    "                for k, v in locals()[fname].items():\n",
    "                    file.write(f\"{k}\\t{v}\\n\")\n",
    "            else:\n",
    "                for v in locals()[fname]:\n",
    "                    file.write(f\"{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-data]",
   "language": "python",
   "name": "conda-env-.conda-data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
